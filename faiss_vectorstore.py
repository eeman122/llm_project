# -*- coding: utf-8 -*-
"""Clean_Version_FAISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186ww6f_lccA3yT9d3BgbfMal86HDrk5_

## Install all dependencies
"""

!pip install -q bitsandbytes accelerate transformers langchain-community sentence-transformers faiss-cpu pandas

"""## Import libraries"""

import os
import glob
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

"""##  Chunking Function"""

def chunk_text_file(file_path, output_folder, chunk_size=500):
    os.makedirs(output_folder, exist_ok=True)
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    base_name = os.path.basename(file_path).rsplit('.', 1)[0]

    for idx, chunk in enumerate(chunks):
        with open(os.path.join(output_folder, f"{base_name}_chunk_{idx}.txt"), 'w', encoding='utf-8') as f:
            f.write(chunk)

    return len(chunks)


def chunk_all_transcripts(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    files = glob.glob(os.path.join(input_folder, '*.txt'))
    for file in files:
        chunk_text_file(file, output_folder)

def chunk_all_transcripts(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    files = glob.glob(os.path.join(input_folder, '*.txt'))
    for file in files:
        chunk_text_file(file, output_folder)

chunk_text_file('/content/Call Centre Agent Evaluation Rulebook.txt', '/content/rulebook_chunks')
#chunk_all_transcripts('/content/50calls_transcripts', '/content/50calls_chunks')

"""## Build and Save FAISS Vectorstore"""

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
chunk_paths = glob.glob('/content/50calls_chunks/*.txt') + glob.glob('/content/rulebook_chunks/*.txt')

documents = []
for path in chunk_paths:
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read().strip()
        if text:
            documents.append(Document(page_content=text))

vectorstore = FAISS.from_documents(documents, embedding=embedding_model)
vectorstore.save_local('/content/faiss_langchain_store')